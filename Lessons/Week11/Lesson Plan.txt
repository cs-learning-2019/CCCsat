1) Review linear regression and gradient descent

2) We will focus on simply NNs for now (mainly MLP). Lets go into detail on how a simply NN is defined and how it works.
	- MLP --> Is Feedforward and has at least three layers (Input and output layers and then 1 or more hidden layers)
	- Each node is basically a neuron. These neurons have activation function. ReLu (Rectified linear unit) and Sigmod are exmaples.
	- Each node also has a weight and some bias
	- So A(wa+b) is the equation for activation of a single neuron
	- An important activation function that can be applied to the output layer is softmax (https://victorzhou.com/blog/softmax/)
	- Our NN will have a cost or loss function (we want to minimize this)
	- How does the NN learn --> Backpropagation (We will ignore the math here)
	- To be super accurate we want to do BP for each piece of data and average all the changes we want to make to each weight and bias
	but that is too expensive so we have mini batches. With mini batches we only do one BP for each mini batch.
	- It is important to shuffle our data
	- There is also the dropout which may help (it prevents overfitting)
	- overfit and underfitting
	- When to stop training? https://machinelearningmastery.com/early-stopping-to-avoid-overtraining-neural-network-models/
	- So why do we want to batch the data and also shuffle it? (https://stats.stackexchange.com/questions/245502/why-should-we-shuffle-data-while-training-a-neural-network/311318#311318)

3) Alright time to get down and dirty. Lets finally look at some code and make our first neural net
